DATA:
  MAX_SAMPLE_INTERVAL: 200
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  SEARCH:
    CENTER_JITTER: 4.5
    FACTOR: 5.0
    SCALE_JITTER: 0.5
    SIZE: 384
  STD:
  - 0.229
  - 0.224
  - 0.225
  TEMPLATE:
    CENTER_JITTER: 0
    FACTOR: 2.0
    SCALE_JITTER: 0
    SIZE: 192
  TRAIN:
    DATASETS_NAME:
      - TNL2K_Lang
      - LASOT_Lang
      - RefCOCO14
      - OTB_Lang
    DATASETS_RATIO:
      - 6
      - 6
      - 6
      - 1
    SAMPLE_PER_EPOCH: 60000
  VAL:
    DATASETS_NAME:
    - GOT10K_votval
    DATASETS_RATIO:
    - 1
    SAMPLE_PER_EPOCH: 10000
MODEL:
  PRETRAIN_FILE: "OSTrack_ep0300.pth.tar" # "mae_pretrain_vit_base.pth" # 
  EXTRA_MERGER: False
  RETURN_INTER: False
  BACKBONE:
    TYPE: vit_base_patch16_224_ce
    STRIDE: 16
    CE_LOC: [3, 6, 9]
    CE_KEEP_RATIO: [0.7, 0.7, 0.7]
    CE_TEMPLATE_RANGE: 'CTR_POINT'  # choose between ALL, CTR_POINT, CTR_REC, GT_BOX
  TEXT_ENCODER: roberta-base  # choose between roberta-base, bert-base, clip
  FREEZE_TEXT_ENCODER: True
  DECODER:
    MEMORY_POSITION_EMBEDDING: sine
    QUERY_POSITION_EMBEDDING: learned
    DEC_LAYERS: 3
    HIDDEN_DIM: 256
    MLP_RATIO: 8
    NUM_HEADS: 8
    DROPOUT: 0.1
    VOCAB_SIZE: 1001
    BBOX_TYPE: 'xyxy'    # choose between 'xyxy' and 'cxcywh'

  VLFUSION_LAYERS: 1       # multi-modal encoder layers
  VL_INPUT_TYPE: 'separate'  # choose between 'separate' and 'concat'
  
  HEAD:
    TYPE: MLP
    NUM_CHANNELS: 256
TRAIN:
  BBOX_TASK: True
  LANGUAGE_TASK: True
  BACKBONE_MULTIPLIER: 0.1
  DROP_PATH_RATE: 0.1
  CE_START_EPOCH: 20  # candidate elimination start epoch
  CE_WARM_EPOCH: 50  # candidate elimination warm up epoch
  BATCH_SIZE: 32
  NUM_WORKER: 2
  EPOCH: 150
  GIOU_WEIGHT: 2.0
  L1_WEIGHT: 5.0
  GRAD_CLIP_NORM: 0.1
  LR: 0.0004
  LR_DROP_EPOCH: 125
  OPTIMIZER: ADAMW
  PRINT_INTERVAL: 50
  SCHEDULER:
    TYPE: step
    DECAY_RATE: 0.1
  VAL_EPOCH_INTERVAL: 1000  # no use val
  WEIGHT_DECAY: 0.0001
  AMP: False
TEST:
  EPOCH: 150
  SEARCH_FACTOR: 5.0
  SEARCH_SIZE: 384
  TEMPLATE_FACTOR: 2.0
  TEMPLATE_SIZE: 192